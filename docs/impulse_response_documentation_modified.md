# Impulse Response Generation Documentation (Modified)

This document provides a detailed explanation of the components responsible for creating and processing impulse responses in the spatial audio simulation system. The impulse response is a critical element that captures how sound propagates and reflects in the virtual environment, enabling spatial audio rendering.

## Table of Contents

1.  [Overview](#overview)
2.  [Key Files and Components](#key-files-and-components)
3.  [Impulse Response Generation Process](#impulse-response-generation-process)
4.  [Early Reflections Processing (Gain + ITD)](#early-reflections-processing-gain--itd)
5.  [Late Reverberation Processing](#late-reverberation-processing)
6.  [Binaural Spatialization (Simplified)](#binaural-spatialization-simplified)
7.  [Audio Playback and Testing](#audio-playback-and-testing)
8.  [Key Algorithms and Techniques](#key-algorithms-and-techniques)

## Overview

The impulse response (IR) represents how sound propagates from a source to a listener in a specific environment. In this spatial audio simulation, the impulse response is generated by combining ray tracing data with acoustic modeling techniques. The system creates a binaural impulse response that captures directional characteristics of early reflections using a simplified Gain + Interaural Time Delay (ITD) model, and the diffuse nature of late reverberation using a statistical model, resulting in a spatial audio experience.

## Key Files and Components

The impulse response generation involves several key files and components:

### Main Coordinator

-   **`main.ts`**: Initiates the impulse response calculation process through the `calculateIR()` method, which coordinates the ray tracing and audio processing steps using `AudioProcessorModified`.

### Ray Tracing

-   **`raytracer.ts`**: Performs the acoustic simulation by tracing rays from the sound source and calculating their reflections in the room. Provides ray hit data (`RayHit` interface) that forms the basis for the impulse response.
-   **`ray.ts`**: Defines the `Ray` class and `FrequencyBands` interface.

### Audio Processing

-   **`audio-processor_modified.ts`**: Central component (`AudioProcessorModified` class) that processes ray hit data into a binaural impulse response. Coordinates early reflections (Gain+ITD, temporal spreading) and late reverberation processing. Includes helper functions (`calculateBalancedSpatialGains`, `calculateITDsamples`) for spatialization.
-   **`diffuse-field-model_modified.ts`**: (`DiffuseFieldModelModified` class) Models the diffuse sound field for late reverberation using Velvet Noise and improved stereo decorrelation.
-   **`feedback-delay-network.ts`**: Implements a Feedback Delay Network (FDN), currently available but not used in the primary IR generation path.

### Visualization

-   **`waveform-renderer.ts`**: Visualizes the generated impulse response in both time domain (waveform) and frequency domain (spectrogram).

## Impulse Response Generation Process

The impulse response generation follows these main steps:

1.  **Ray Tracing Simulation**:
    -   The `calculateIR()` method in `main.ts` initiates the process by calling `rayTracer.calculateRayPaths()`.
    -   The ray tracer simulates sound propagation, collecting `RayHit` data (position, time, energy, bounces, etc.).

2.  **Audio Processing**:
    -   Ray hit data is passed to `audioProcessor.processRayHits()` (where `audioProcessor` is an instance of `AudioProcessorModified`).
    -   The `processRayHitsInternal()` method processes the hits into a stereo impulse response.
    -   Hits are sorted by time.
    -   Early reflections (e.g., < 100ms) are processed using the Gain + ITD model and temporal spreading.
    -   Late reverberation (e.g., >= 100ms) is generated using `DiffuseFieldModelModified`.

3.  **Impulse Response Assembly**:
    -   Early reflections and late reverberation are combined with a crossfade (e.g., 80-120ms).
    -   The final impulse response is normalized and stored in an `AudioBuffer`.
    -   The impulse response is visualized using the waveform renderer.

4.  **Audio Playback**:
    -   The impulse response can be tested by convolving it with various test signals (e.g., using `playConvolvedClick`).
    -   The convolved audio is played back through the Web Audio API.

## Early Reflections Processing (Gain + ITD)

Early reflections (typically the first 100ms) are processed using the following approach within `processRayHitsInternal`:

1.  **Sorting by Arrival Time**: Hits are sorted by `hit.time`.
2.  **Amplitude Calculation**:
    -   Total energy across frequency bands is calculated.
    -   Amplitude is derived from energy, with attenuation based on bounce count (`Math.exp(-hit.bounces * 0.2)`) and an overall loudness scaling factor (`loudnessScale = 0.05`).
3.  **Direction Calculation**: Azimuth and elevation angles (radians) are calculated from the hit position relative to the listener's position and orientation.
4.  **Gain Calculation**:
    -   The `calculateBalancedSpatialGains` helper function calculates `leftGain` and `rightGain` based on azimuth, elevation, and distance, using a balanced panning approach with distance/elevation/back-factor adjustments.
5.  **ITD Calculation**:
    -   The `calculateITDsamples` helper function calculates the Interaural Time Delay in samples based on azimuth using Woodworth's formula approximation.
    -   The sample delay for the left and right ears (`leftDelaySamples`, `rightDelaySamples`) is determined based on the sign of the ITD.
6.  **Temporal Spreading & Application**:
    -   The calculated `amplitude * gain` is not applied as a single sample (Dirac delta).
    -   Instead, it's spread over a short duration (e.g., 20ms) using an exponential decay (`spreadGain = Math.exp(-k / decayConstant)`).
    -   Linear interpolation is used to apply the spread impulse accurately at fractional sample delays determined by the ITD (`baseLeftIndex + k`, `baseRightIndex + k`).
    -   The spread, gain-adjusted impulse is added to the main `leftIR` and `rightIR` buffers.

```typescript
// Simplified conceptual flow within the early hits loop:

// 1. Calculate amplitude (energy, bounces, loudnessScale)
const amplitude = calculateAmplitude(hit.energies, hit.bounces, loudnessScale);

// 2. Calculate direction (azimuthRad, elevationRad) and distance
const { azimuthRad, elevationRad, distance } = calculateDirection(hit.position, listener);

// 3. Calculate Gains
const [leftGain, rightGain] = calculateBalancedSpatialGains(azimuthRad, elevationRad, distance);

// 4. Calculate ITD
const itd_samples = calculateITDsamples(azimuthRad, sampleRate);
const { leftDelaySamples, rightDelaySamples } = determineDelays(itd_samples);

// 5. Apply Spread Impulse with Fractional Delay
const baseLeftIndex = sampleIndex + leftDelaySamples;
const baseRightIndex = sampleIndex + rightDelaySamples;
applySpreadImpulseWithInterpolation(leftIR, baseLeftIndex, amplitude * leftGain, spreadConfig);
applySpreadImpulseWithInterpolation(rightIR, baseRightIndex, amplitude * rightGain, spreadConfig);

```

## Late Reverberation Processing

Late reverberation (after 100ms) is primarily handled by `DiffuseFieldModelModified`:

1.  **RT60 Calculation**: Frequency-dependent Reverberation Times (RT60) are calculated based on room volume, surface area, and mean material absorption using Sabine's formula (`calculateRT60Values`).
2.  **Diffuse Field Generation**:
    -   For each frequency band, a diffuse impulse response is generated using the Velvet Noise algorithm (`generateVelvetNoise`). This algorithm creates a sparse sequence of impulses whose density and decay match the calculated RT60 and room properties.
3.  **Frequency Filtering/Combination**: The individual frequency band responses are combined into a single mono IR, applying frequency-dependent gains (`applyFrequencyFiltering`).
4.  **Stereo Decorrelation**: The mono late reverb IR is processed to create distinct left and right channels, adding spaciousness. The current method uses small, randomized delays and slight filtering differences between the channels (`processLateReverberation`).
5.  **Crossfading**: In `AudioProcessorModified`, the generated stereo late reverberation is crossfaded with the processed early reflections (e.g., between 80ms and 120ms) to create the final combined IR. A gain factor (`lateReverbGain`) can adjust the level of the late reverb during this stage.

## Binaural Spatialization (Simplified)

The primary method for creating directional cues for early reflections is the **Gain + ITD model**:

-   **Interaural Level Difference (ILD):** Simulated by applying different gains (`leftGain`, `rightGain`) to the left and right channels based on direction and distance, calculated by `calculateBalancedSpatialGains`.
-   **Interaural Time Difference (ITD):** Simulated by applying small time delays (`leftDelaySamples`, `rightDelaySamples`) to the left or right channel based on direction, calculated by `calculateITDsamples`. Fractional sample delays are handled using linear interpolation.

This provides basic directional cues but lacks the detailed frequency filtering of full HRTF convolution, which is crucial for realistic elevation perception and externalization (making the sound feel "outside" the head).

## Audio Playback and Testing

The final binaural impulse response is stored in an `AudioBuffer`. The `AudioProcessorModified` class provides methods for testing:

-   `playConvolvedClick()`: Convolves a generated click sound with the IR.
-   `playConvolvedSineWave()`: Convolves a sine wave with the IR.
-   `playNoiseWithIR()`: Convolves generated noise (with decay) with the IR.
-   `visualizeImpulseResponse()`: Sends the IR data to `WaveformRenderer` for display.

Convolution is performed using the Web Audio API's `ConvolverNode`.

## Key Algorithms and Techniques

The impulse response generation employs several algorithms and techniques:

### 1. Stochastic Ray Tracing

Used by `RayTracer` to simulate sound propagation, calculating reflections and energy loss to generate `RayHit` data.

### 2. Gain + ITD Model

Used for spatializing early reflections. Combines Interaural Level Differences (ILD) and Interaural Time Differences (ITD) calculated from ray arrival direction and distance.

### 3. Temporal Spreading

Applied to early reflection impulses to smooth them over a short duration (e.g., 20ms) using an exponential decay, reducing perceived "clickiness". Uses linear interpolation for fractional sample delays.

### 4. Velvet Noise Algorithm

Used in `DiffuseFieldModelModified` to efficiently generate the diffuse late reverberation tail based on statistical room properties and RT60 values.

### 5. Stereo Decorrelation

Applied to the late reverberation tail in `DiffuseFieldModelModified` to create a sense of spaciousness using small delays and filtering differences between channels.

### 6. Frequency-Dependent Processing

Acoustic phenomena like absorption are modeled per frequency band in the ray tracer. Late reverberation RT60 is calculated per band, and bands are combined with frequency-dependent gains.

### 7. Exponential Decay Envelopes

Used implicitly in the Velvet Noise amplitude calculation and potentially in `calculateDecayCurve` for noise generation, modeling natural energy decay.